{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing neccesary libs, data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as  sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('dataset/creditcard.csv')\n",
    "colors = sns.color_palette(\"RdPu\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling amount and time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "#df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data before creating the subsamples\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "undersampled_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "# undersampled_df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\n",
    "v14_fraud = undersampled_df['V14'].loc[undersampled_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
    "v14_iqr = q75 - q25\n",
    "\n",
    "v14_cut_off = v14_iqr * 1.5\n",
    "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
    "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
    "undersampled_df = undersampled_df.drop(undersampled_df[(undersampled_df['V14'] > v14_upper) | (undersampled_df['V14'] < v14_lower)].index)\n",
    "\n",
    "# -----> V12 removing outliers from fraud transactions\n",
    "v12_fraud = undersampled_df['V12'].loc[undersampled_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
    "v12_iqr = q75 - q25\n",
    "\n",
    "v12_cut_off = v12_iqr * 1.5\n",
    "v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n",
    "undersampled_df = undersampled_df.drop(undersampled_df[(undersampled_df['V12'] > v12_upper) | (undersampled_df['V12'] < v12_lower)].index)\n",
    "\n",
    "# -----> V10 removing outliers from fraud transactions\n",
    "v10_fraud = undersampled_df['V10'].loc[undersampled_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
    "v10_iqr = q75 - q25\n",
    "\n",
    "v10_cut_off = v10_iqr * 1.5\n",
    "v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n",
    "outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
    "undersampled_df = undersampled_df.drop(undersampled_df[(undersampled_df['V10'] > v10_upper) | (undersampled_df['V10'] < v10_lower)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Clustering for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atik_\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\atik_\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# T-SNE Implementation\n",
    "X_undersampled = undersampled_df.drop('Class', axis=1)\n",
    "y_undersampled = undersampled_df['Class']\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_undersampled.values)\n",
    "\n",
    "# PCA Implementation\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X_undersampled.values)\n",
    "\n",
    "# TruncatedSVD\n",
    "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X_undersampled.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:-1], df.iloc[:,-1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling imbalanced data using RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled_ros, y_oversampled_ros = ros.fit_resample(X,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling using SMOTE Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_oversampled_smote, y_oversampled_smote = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bb1e9e58b40c4ac6928b6d4e9ed2244fea74d0168904c0a47896057ac3e03f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
